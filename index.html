<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>
    Real-Time 3D Pedestrian Simulation from 2D Trajectories
  </title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f4f4f4;
      color: #333;
      max-width: 900px;
      margin: auto;
      padding: 2em;
    }
    h1, h2 {
      color: #2c3e50;
    }
    .gif-container {
      text-align: center;
      margin: 2em 0;
    }
    .tech-list, .dataset-list {
      list-style-type: square;
      margin-left: 2em;
    }
    a {
      color: #007acc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .citation {
      font-style: italic;
      font-size: 0.95em;
    }
  </style>
</head>
<body>
  <p><a href="https://amartaivan-s.github.io/">🏠 Home</a></p>
  <h1>Reconstructing Real-Time 3D Pedestrian Motion from 2D Trajectories</h1>
  <h2 style="color:#555; font-weight:normal;">
    Reconstructing Movement in 3D Space Using Multi-Pedestrian Tracking Data and Interactive 3D Rendering
  </h2>

  <h2>🔍 Project Overview</h2>
  <p>
    This project reconstructs and visualizes real-world multi-pedestrian trajectories in a fully interactive 3D environment. 
    Unlike conventional static plots, it employs <strong>PyQtGraph's OpenGL module</strong> to render pedestrians as customizable humanoid models, including separate body, head, arms, and legs. 
    Environmental geometry such as walls, exits, and pillars is incorporated for realistic spatial context. 
    The visualization runs in near real-time, allowing for exploration of crowd behaviors, space usage, and model validation.
  </p>

  
<div class="gif-container" style="max-width: 800px; margin: 2em auto;">
  <video autoplay muted loop playsinline style="width: 100%; height: auto; border-radius: 6px; box-shadow: 0 4px 12px rgba(0,0,0,0.2);">
    <source src="3d-pedestrians.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <p><small>Figure: Sample 3D simulation of pedestrians with custom humanoid models in an environment.</small></p>
</div>


  <h2>🧰 Technologies Used</h2>
  <ul class="tech-list">
     <li>
    <strong>Python</strong> – core logic, data loading, and agent control
  </li>
  <li>
    <a href="https://www.pyqtgraph.org/" target="_blank"><strong>PyQtGraph</strong></a> – 2D/3D plotting and OpenGL-based visualization
    (<a href="https://pyqtgraph.readthedocs.io/en/latest/getting_started/3dgraphics.html" target="_blank">3D Graphics Docs</a>)
  </li>
  </ul>

  <h2>📦 Dataset</h2>
  <ul class="dataset-list">
    <li>
      <a href="https://dil.atr.jp/ISL/crest2010_HRI/ATC_dataset/" target="_blank">
        ATC Pedestrian Trajectory Dataset
      </a> – Real-world shopping mall pedestrian trajectories collected by ATC, Japan
    </li>
  </ul>
  <p class="citation">
    D. Brscic, T. Kanda, T. Ikeda, T. Miyashita, "Person position and body direction tracking in large public spaces using 3D range sensors", IEEE Transactions on Human-Machine Systems, Vol. 43, No. 6, pp. 522-534, 2013
  </p>

  <h2>🎯 Goals & Applications</h2>
  <ul class="tech-list">
    <li>Model and reproduce realistic pedestrian behaviors in 3D space</li>
    <li>Render interactive humanoid agents using PyQtGraph OpenGL</li>
    <li>Analyze spatial interactions and crowd density in real environments</li>
    <li>Support research on evacuation modeling, AI behavior, and space optimization</li>
  </ul>

  <h2>🚧 Current Status</h2>
  <p>
    ✔️ Customized 3D humanoid agents created (body, head, arms, legs)<br/>
    ✔️ Real-time trajectory playback using PyQtGraph OpenGL 3D<br/>
    🔄 Implementing environment import, collision detection, and user interaction controls<br/>
    📈 Planning to integrate statistical analysis tools

<footer style="margin-top: 3em; font-size: 0.85em; color: #666;">
    Last updated: Oct 2025 • Author: Sanjjamts Amartaivan
  </footer>

  </body>
</html>
